# main.py
"""
AI Vector Search Demo (Elasticsearch)
=====================================
Full-featured Q&A system v·ªõi:
- Multi-level retrieval (Level 0, 1, 2...)
- Structured prompt builder with custom prompts
- "Tell me more" functionality
- File management (upload with streaming, replace, delete)
- Buffer 10-20% for better retrieval
"""
import os
import uuid
from datetime import datetime
from typing import List
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

from config import settings
from vector.elastic_client import init_index, es
from services.splitter import split_into_sentences
from services.retriever import (
    index_sentences, 
    index_sentences_batch,
    get_top_unique_sentences_grouped,
    get_sentences_by_level,
    get_max_level,
    delete_all_documents,
    get_document_count
)
from services.prompt_builder import (
    generate_question_variants,
    extract_keywords,
    build_final_prompt,
    call_llm,
)
from services.session_manager import session_manager
from models.request_models import (
    AskRequest, 
    AskResponse, 
    ContinueRequest, 
    ContinueResponse,
    UploadResponse,
    DocumentStats,
    HealthResponse,
    ErrorResponse
)

app = FastAPI(
    title="AI Vector Search Demo (Elasticsearch)",
    description="""
## ü§ñ H·ªá th·ªëng Q&A th√¥ng minh v·ªõi Multi-level Retrieval

### üìã T·ªïng quan
H·ªá th·ªëng s·ª≠ d·ª•ng Elasticsearch l√†m vector database, OpenAI cho embeddings v√† chat.
H·ªó tr·ª£ **7 modules ch√≠nh** theo y√™u c·∫ßu client:

### ‚úÖ C√°c Modules:
1. **File Upload** - Upload file .txt v·ªõi streaming (tr√°nh tr√†n RAM)
2. **Sentence Embeddings** - T·∫°o vector embeddings cho t·ª´ng c√¢u
3. **Query Processing** - X·ª≠ l√Ω c√¢u h·ªèi v·ªõi buffer 10-20%
4. **Deduplication** - Lo·∫°i b·ªè c√¢u tr√πng l·∫∑p
5. **Prompt Builder** - X√¢y d·ª±ng prompt c√≥ c·∫•u tr√∫c + custom prompts
6. **Response Generation** - Sinh c√¢u tr·∫£ l·ªùi t·ª´ LLM
7. **"Tell me more"** - ƒê√†o s√¢u v√†o c√°c levels ti·∫øp theo

### üîÑ Flow ho·∫°t ƒë·ªông:
```
POST /upload ‚Üí T√°ch c√¢u ‚Üí Embedding ‚Üí L∆∞u ES (by level)
POST /ask ‚Üí Vector search + Buffer ‚Üí Prompt Builder ‚Üí LLM ‚Üí Response + session_id
POST /continue ‚Üí D√πng session_id ‚Üí Level ti·∫øp theo ‚Üí Expand answer
```

### üí° Features n·ªïi b·∫≠t:
- **Buffer 10-20%**: L·∫•y th√™m c√¢u d·ª± ph√≤ng ƒë·ªÉ c·∫£i thi·ªán k·∫øt qu·∫£
- **Custom Prompts**: Ng∆∞·ªùi d√πng c√≥ th·ªÉ th√™m instructions ri√™ng
- **Streaming Upload**: ƒê·ªçc file theo chunks ƒë·ªÉ tr√°nh tr√†n RAM
- **Session Management**: Theo d√µi cu·ªôc h·ªôi tho·∫°i cho "Tell me more"
    """,
    version="2.0.0",
    openapi_tags=[
        {
            "name": "üìÅ File Management",
            "description": "Upload, replace, v√† qu·∫£n l√Ω documents trong Elasticsearch"
        },
        {
            "name": "‚ùì Q&A",
            "description": "H·ªèi ƒë√°p v·ªõi multi-level retrieval v√† custom prompts"
        },
        {
            "name": "üìä Info",
            "description": "Health check v√† th√¥ng tin h·ªá th·ªëng"
        }
    ],
    responses={
        400: {"model": ErrorResponse, "description": "Bad Request"},
        404: {"model": ErrorResponse, "description": "Not Found"},
        500: {"model": ErrorResponse, "description": "Internal Server Error"}
    }
)

# CORS cho d·ªÖ test t·ª´ frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


# Kh·ªüi t·∫°o index khi app start
@app.on_event("startup")
def startup_event():
    init_index()


# ============================================================
# MODULE 1: File Management
# ============================================================

CHUNK_SIZE = 1024 * 1024  # 1MB chunks for streaming


@app.post(
    "/upload",
    response_model=UploadResponse,
    tags=["üìÅ File Management"],
    summary="Upload v√† index file .txt",
    description="""
## Upload file vƒÉn b·∫£n

### ‚öôÔ∏è X·ª≠ l√Ω:
1. **Streaming read** - ƒê·ªçc file theo chunks (1MB) ƒë·ªÉ tr√°nh tr√†n RAM
2. **Sentence splitting** - T√°ch th√†nh c√¢u ri√™ng l·∫ª
3. **Level assignment** - M·ªói 5 c√¢u = 1 level
4. **Batch embedding** - T·∫°o embeddings theo batch (hi·ªáu qu·∫£)
5. **Elasticsearch indexing** - L∆∞u v√†o vector database

### üìä K·∫øt qu·∫£ tr·∫£ v·ªÅ:
- `file_id`: ID duy nh·∫•t c·ªßa file
- `filename`: T√™n file g·ªëc
- `total_sentences`: S·ªë c√¢u ƒë√£ index
- `max_level`: Level cao nh·∫•t (ƒë·ªÉ bi·∫øt c√≥ bao nhi√™u levels cho "Tell me more")
- `buffer_info`: Th√¥ng tin v·ªÅ kh·∫£ nƒÉng buffer

### ‚ö†Ô∏è L∆∞u √Ω:
- Ch·ªâ h·ªó tr·ª£ file `.txt`
- Encoding: UTF-8 ho·∫∑c Latin-1 (auto-detect)
    """,
    responses={
        200: {
            "description": "File uploaded th√†nh c√¥ng",
            "content": {
                "application/json": {
                    "example": {
                        "file_id": "550e8400-e29b-41d4-a716-446655440000",
                        "filename": "document.txt",
                        "total_sentences": 50,
                        "max_level": 10,
                        "message": "File processed successfully. 50 sentences indexed across 11 levels."
                    }
                }
            }
        },
        400: {"description": "Invalid file type ho·∫∑c file r·ªóng"}
    }
)
async def upload_file(
    file: UploadFile = File(
        ..., 
        description="File .txt c·∫ßn upload. K√≠ch th∆∞·ªõc t·ªëi ƒëa khuy·∫øn ngh·ªã: 10MB"
    )
):
    """
    Upload file .txt v·ªõi streaming read ƒë·ªÉ t·ªëi ∆∞u RAM.
    
    File s·∫Ω ƒë∆∞·ª£c:
    - ƒê·ªçc theo chunks 1MB
    - T√°ch th√†nh c√¢u
    - T·∫°o embeddings theo batch
    - Index v√†o Elasticsearch v·ªõi level
    """
    if not file.filename.endswith(".txt"):
        raise HTTPException(
            status_code=400, 
            detail="Only .txt files are supported. Please convert your document to .txt format."
        )

    # Streaming read ƒë·ªÉ tr√°nh tr√†n RAM v·ªõi file l·ªõn
    chunks = []
    total_size = 0
    MAX_SIZE = 50 * 1024 * 1024  # 50MB limit
    
    while True:
        chunk = await file.read(CHUNK_SIZE)
        if not chunk:
            break
        chunks.append(chunk)
        total_size += len(chunk)
        
        if total_size > MAX_SIZE:
            raise HTTPException(
                status_code=400,
                detail=f"File too large. Maximum size is 50MB. Your file: {total_size / (1024*1024):.1f}MB"
            )
    
    content_bytes = b"".join(chunks)
    
    try:
        text = content_bytes.decode("utf-8")
    except UnicodeDecodeError:
        text = content_bytes.decode("latin-1")

    sentences = split_into_sentences(text)
    if not sentences:
        raise HTTPException(
            status_code=400, 
            detail="No valid sentences found in file. Make sure the file contains readable text."
        )

    # Index sentences v·ªõi batch processing v√† l·∫•y max_level
    file_id = str(uuid.uuid4())
    max_level = index_sentences_batch(sentences, file_id=file_id, batch_size=20)

    return UploadResponse(
        file_id=file_id,
        filename=file.filename,
        total_sentences=len(sentences),
        max_level=max_level,
        message=f"File processed successfully. {len(sentences)} sentences indexed across {max_level + 1} levels.",
        buffer_info=f"With 15% buffer, queries can retrieve up to {int(15 * 1.15)} sentences"
    )


@app.post(
    "/replace",
    response_model=UploadResponse,
    tags=["üìÅ File Management"],
    summary="Thay th·∫ø to√†n b·ªô d·ªØ li·ªáu",
    description="""
## Thay th·∫ø d·ªØ li·ªáu hi·ªán t·∫°i b·∫±ng file m·ªõi

### ‚öôÔ∏è X·ª≠ l√Ω:
1. **X√≥a t·∫•t c·∫£** documents c≈© trong Elasticsearch
2. **Upload v√† index** file m·ªõi

### ‚ö†Ô∏è C·∫£nh b√°o:
- H√†nh ƒë·ªông n√†y KH√îNG TH·ªÇ ho√†n t√°c
- T·∫•t c·∫£ sessions hi·ªán t·∫°i s·∫Ω b·ªã invalid
    """
)
async def replace_file(
    file: UploadFile = File(..., description="File .txt m·ªõi ƒë·ªÉ thay th·∫ø")
):
    """Thay th·∫ø to√†n b·ªô d·ªØ li·ªáu b·∫±ng file m·ªõi."""
    # X√≥a d·ªØ li·ªáu c≈©
    delete_all_documents()
    
    # Clear t·∫•t c·∫£ sessions
    session_manager.clear_all()
    
    # Upload file m·ªõi
    return await upload_file(file)


@app.delete(
    "/documents",
    tags=["üìÅ File Management"],
    summary="X√≥a t·∫•t c·∫£ documents",
    description="""
## X√≥a to√†n b·ªô d·ªØ li·ªáu

### ‚ö†Ô∏è C·∫£nh b√°o:
- H√†nh ƒë·ªông n√†y KH√îNG TH·ªÇ ho√†n t√°c
- C·∫ßn upload file m·ªõi tr∆∞·ªõc khi s·ª≠ d·ª•ng /ask
    """,
    responses={
        200: {
            "description": "X√≥a th√†nh c√¥ng",
            "content": {
                "application/json": {
                    "example": {"message": "All documents deleted successfully", "documents_deleted": 50}
                }
            }
        }
    }
)
async def delete_all():
    """X√≥a t·∫•t c·∫£ documents trong Elasticsearch."""
    count = get_document_count()
    success = delete_all_documents()
    session_manager.clear_all()
    
    if success:
        return {"message": "All documents deleted successfully", "documents_deleted": count}
    raise HTTPException(status_code=500, detail="Failed to delete documents")


@app.get(
    "/documents/count",
    response_model=DocumentStats,
    tags=["üìÅ File Management"],
    summary="L·∫•y th·ªëng k√™ documents",
    description="""
## Th·ªëng k√™ documents trong Elasticsearch

### üìä Tr·∫£ v·ªÅ:
- `total_documents`: T·ªïng s·ªë c√¢u ƒë√£ index
- `max_level`: Level cao nh·∫•t
- `levels_available`: S·ªë levels c√≥ th·ªÉ d√πng cho "Tell me more"
- `ready`: True n·∫øu c√≥ d·ªØ li·ªáu, s·∫µn s√†ng nh·∫≠n c√¢u h·ªèi
    """
)
async def get_count():
    """L·∫•y th·ªëng k√™ documents hi·ªán c√≥."""
    count = get_document_count()
    max_level = get_max_level()
    return DocumentStats(
        total_documents=count,
        max_level=max_level,
        levels_available=max_level + 1 if count > 0 else 0,
        ready=count > 0
    )


# ============================================================
# MODULE 2-6: Ask Question (First Query)
# ============================================================

@app.post(
    "/ask",
    response_model=AskResponse,
    tags=["‚ùì Q&A"],
    summary="ƒê·∫∑t c√¢u h·ªèi",
    description="""
## H·ªèi ƒë√°p v·ªõi Multi-level Retrieval

### üîÑ Flow x·ª≠ l√Ω:
1. **Vector Search** - T√¨m c√¢u ngu·ªìn li√™n quan (v·ªõi buffer 10-20%)
2. **Deduplicate** - Lo·∫°i b·ªè c√¢u tr√πng l·∫∑p
3. **Generate Variants** - T·∫°o 3-4 bi·∫øn th·ªÉ c√¢u h·ªèi
4. **Extract Keywords** - Gi·∫£i nghƒ©a keywords quan tr·ªçng
5. **Build Prompt** - X√¢y d·ª±ng prompt c√≥ c·∫•u tr√∫c + custom instructions
6. **Call LLM** - Sinh c√¢u tr·∫£ l·ªùi

### üì• Parameters:
- `query` (required): C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
- `limit`: S·ªë c√¢u ngu·ªìn t·ªëi ƒëa (default: 15)
- `buffer_percentage`: % c√¢u d·ª± ph√≤ng (10-20%)
- `custom_prompt`: Instructions t√πy ch·ªânh t·ª´ ng∆∞·ªùi d√πng

### üì§ Response:
- `session_id`: D√πng cho /continue (Tell me more)
- `answer`: C√¢u tr·∫£ l·ªùi t·ª´ LLM
- `source_sentences`: C√°c c√¢u ngu·ªìn ƒë√£ s·ª≠ d·ª•ng
- `can_continue`: True n·∫øu c√≥ th·ªÉ ƒë√†o s√¢u th√™m

### üí° Tips:
- S·ª≠ d·ª•ng `buffer_percentage=15` ƒë·ªÉ c√¢n b·∫±ng ƒë·ªô ch√≠nh x√°c v√† ƒë·ªô ƒëa d·∫°ng
- Th√™m `custom_prompt` ƒë·ªÉ ƒëi·ªÅu ch·ªânh style/format c√¢u tr·∫£ l·ªùi
    """,
    responses={
        200: {
            "description": "C√¢u tr·∫£ l·ªùi th√†nh c√¥ng",
            "content": {
                "application/json": {
                    "example": {
                        "session_id": "abc-123",
                        "answer": "D·ª±a tr√™n th√¥ng tin t√¨m ƒë∆∞·ª£c...",
                        "question_variants": "1. What is X?\n2. Explain X...",
                        "source_sentences": [{"text": "Sample sentence", "level": 0, "score": 0.95}],
                        "current_level": 0,
                        "max_level": 5,
                        "can_continue": True
                    }
                }
            }
        }
    }
)
async def ask(req: AskRequest):
    """
    Nh·∫≠n c√¢u h·ªèi t·ª´ user, th·ª±c hi·ªán full flow v·ªõi buffer support.
    
    H·ªó tr·ª£ custom_prompt ƒë·ªÉ ng∆∞·ªùi d√πng c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh
    c√°ch LLM tr·∫£ l·ªùi (format, style, language, etc.)
    """
    # Ki·ªÉm tra c√≥ data kh√¥ng
    if get_document_count() == 0:
        raise HTTPException(
            status_code=404, 
            detail="No documents found. Please upload a file first using POST /upload"
        )
    
    # 1. L·∫•y c√¢u ngu·ªìn t·ª´ Elasticsearch v·ªõi buffer support
    source_sentences = get_top_unique_sentences_grouped(
        req.query, 
        limit=req.limit,
        buffer_percentage=req.buffer_percentage
    )
    if not source_sentences:
        raise HTTPException(
            status_code=404, 
            detail="No source sentences found matching your query. Try rephrasing your question."
        )

    # 2. T·∫°o bi·∫øn th·ªÉ c√¢u h·ªèi + gi·∫£i nghƒ©a keyword
    question_variants = generate_question_variants(req.query)
    keyword_meaning = extract_keywords(req.query)

    # 3. Build final prompt v·ªõi custom_prompt support

    # 3. Build final prompt v·ªõi custom_prompt support
    prompt = build_final_prompt(
        user_query=req.query,
        question_variants=question_variants,
        keyword_meaning=keyword_meaning,
        source_sentences=source_sentences,
        continue_mode=False,
        custom_prompt=req.custom_prompt
    )

    # 4. G·ªçi LLM
    answer = call_llm(prompt)
    
    # 5. T·∫°o session ƒë·ªÉ track conversation
    max_level = get_max_level()
    session = session_manager.create_session(req.query, max_level)
    
    # C·∫≠p nh·∫≠t session v·ªõi c√°c c√¢u ƒë√£ d√πng
    used_texts = [s["text"] for s in source_sentences]
    session_manager.update_session(
        session.session_id,
        used_sentences=used_texts,
        question_variants=question_variants,
        keywords=keyword_meaning
    )
    
    # T√≠nh current_level t·ª´ source sentences
    current_level = max(s["level"] for s in source_sentences) if source_sentences else 0
    
    # T√≠nh s·ªë c√¢u th·ª±c t·∫ø ƒë∆∞·ª£c retrieve v·ªõi buffer
    buffer_applied = req.buffer_percentage if req.buffer_percentage else 0

    return AskResponse(
        session_id=session.session_id,
        answer=answer,
        question_variants=question_variants,
        keyword_meaning=keyword_meaning,
        source_sentences=source_sentences,
        current_level=current_level,
        max_level=max_level,
        prompt_used=prompt,
        can_continue=current_level < max_level,
        sentences_retrieved=len(source_sentences),
        buffer_applied=buffer_applied
    )


# ============================================================
# MODULE 7: Continue / Tell me more
# ============================================================

@app.post(
    "/continue",
    response_model=ContinueResponse,
    tags=["‚ùì Q&A"],
    summary="Tell me more - ƒê√†o s√¢u th√™m",
    description="""
## M·ªü r·ªông c√¢u tr·∫£ l·ªùi v·ªõi th√¥ng tin t·ª´ levels s√¢u h∆°n

### üîÑ Flow x·ª≠ l√Ω:
1. **Get Session** - L·∫•y th√¥ng tin t·ª´ session_id
2. **Increase Level** - Chuy·ªÉn sang Level 1, 2, 3...
3. **Get NEW sentences** - L·∫•y c√¢u ngu·ªìn M·ªöI (exclude ƒë√£ d√πng)
4. **Generate NEW variants** - T·∫°o bi·∫øn th·ªÉ c√¢u h·ªèi M·ªöI
5. **Update Keywords** - B·ªï sung keywords m·ªõi
6. **Build Prompt** - Prompt cho ch·∫ø ƒë·ªô continue + custom instructions
7. **Call LLM** - Sinh c√¢u tr·∫£ l·ªùi m·ªü r·ªông

### üì• Parameters:
- `session_id` (required): ID t·ª´ response c·ªßa /ask
- `custom_prompt`: Instructions t√πy ch·ªânh b·ªï sung
- `buffer_percentage`: % c√¢u d·ª± ph√≤ng (10-20%)

### üì§ Response:
- T∆∞∆°ng t·ª± /ask nh∆∞ng v·ªõi th√¥ng tin t·ª´ levels s√¢u h∆°n
- `can_continue`: False khi ƒë√£ h·∫øt levels

### üí° Usage Pattern:
```
1. POST /ask ‚Üí get session_id
2. POST /continue v·ªõi session_id ‚Üí get more info
3. Repeat POST /continue until can_continue=false
```
    """,
    responses={
        200: {"description": "C√¢u tr·∫£ l·ªùi m·ªü r·ªông th√†nh c√¥ng"},
        404: {"description": "Session kh√¥ng t·ªìn t·∫°i ho·∫∑c ƒë√£ h·∫øt h·∫°n"},
        400: {"description": "ƒê√£ h·∫øt levels ƒë·ªÉ ƒë√†o s√¢u"}
    }
)
async def continue_conversation(req: ContinueRequest):
    """
    "Tell me more" - ƒê√†o s√¢u v√†o c√°c level ti·∫øp theo v·ªõi buffer support.
    """
    # L·∫•y session
    session = session_manager.get_session(req.session_id)
    if not session:
        raise HTTPException(
            status_code=404,
            detail="Session not found or expired (30 min timeout). Please ask a new question with POST /ask"
        )
    
    # Ki·ªÉm tra c√≥ th·ªÉ continue kh√¥ng
    if session.current_level >= session.max_level_available:
        raise HTTPException(
            status_code=400,
            detail="No more levels available. All information has been explored. Start a new question with POST /ask"
        )
    
    # TƒÉng level
    next_level = session.current_level + 1
    
    # L·∫•y c√¢u ngu·ªìn t·ª´ level m·ªõi (exclude c√°c c√¢u ƒë√£ d√πng) v·ªõi buffer
    source_sentences = get_sentences_by_level(
        query=session.original_query,
        start_level=next_level,
        limit=req.limit if req.limit else 15,
        exclude_texts=session.used_sentences,
        buffer_percentage=req.buffer_percentage
    )
    
    if not source_sentences:
        raise HTTPException(
            status_code=404,
            detail=f"No new sentences found at Level {next_level}. Try asking a different question."
        )
    
    # T·∫°o bi·∫øn th·ªÉ c√¢u h·ªèi M·ªöI (kh√¥ng l·∫∑p v·ªõi c√°c l·∫ßn tr∆∞·ªõc)
    question_variants = generate_question_variants(
        session.original_query,
        previous_variants=session.used_variants,
        continue_mode=True
    )
    
    # Update keyword meaning (t√¨m keywords m·ªõi/s√¢u h∆°n)
    keyword_meaning = extract_keywords(
        session.original_query,
        previous_keywords=session.previous_keywords,
        continue_mode=True
    )
    
    # Build prompt m·ªõi v·ªõi custom_prompt support
    prompt = build_final_prompt(
        user_query=session.original_query,
        question_variants=question_variants,
        keyword_meaning=keyword_meaning,
        source_sentences=source_sentences,
        continue_mode=True,
        continue_count=session.continue_count + 1,
        custom_prompt=req.custom_prompt
    )
    
    # G·ªçi LLM
    answer = call_llm(prompt)
    
    # C·∫≠p nh·∫≠t session
    used_texts = [s["text"] for s in source_sentences]
    session_manager.update_session(
        session.session_id,
        used_sentences=used_texts,
        question_variants=question_variants,
        keywords=keyword_meaning,
        increment_level=True
    )
    
    # T√≠nh current_level v√† buffer info
    current_level = max(s["level"] for s in source_sentences) if source_sentences else next_level
    buffer_applied = req.buffer_percentage if req.buffer_percentage else 0
    
    return ContinueResponse(
        session_id=session.session_id,
        answer=answer,
        question_variants=question_variants,
        keyword_meaning=keyword_meaning,
        source_sentences=source_sentences,
        current_level=current_level,
        max_level=session.max_level_available,
        prompt_used=prompt,
        can_continue=current_level < session.max_level_available,
        continue_count=session.continue_count + 1,
        sentences_retrieved=len(source_sentences),
        buffer_applied=buffer_applied
    )


# ============================================================
# Health & Info Endpoints
# ============================================================

@app.get(
    "/",
    tags=["üìä Info"],
    summary="Th√¥ng tin API",
    description="Tr·∫£ v·ªÅ th√¥ng tin t·ªïng quan v·ªÅ API v√† c√°c endpoints"
)
async def root():
    """Th√¥ng tin t·ªïng quan v·ªÅ API."""
    return {
        "message": "ü§ñ AI Vector Search Demo with Elasticsearch",
        "version": "2.0.0",
        "docs": "/docs",
        "redoc": "/redoc",
        "features": [
            "‚úÖ Multi-level retrieval (Level 0, 1, 2...)",
            "‚úÖ Structured prompt builder",
            "‚úÖ Custom prompts support",
            "‚úÖ Buffer 10-20% for better retrieval",
            "‚úÖ Tell me more functionality",
            "‚úÖ Streaming file upload",
            "‚úÖ File management"
        ],
        "endpoints": {
            "file_management": {
                "POST /upload": "Upload file .txt (streaming)",
                "POST /replace": "Thay th·∫ø to√†n b·ªô d·ªØ li·ªáu",
                "DELETE /documents": "X√≥a t·∫•t c·∫£",
                "GET /documents/count": "Th·ªëng k√™ documents"
            },
            "qa": {
                "POST /ask": "H·ªèi c√¢u h·ªèi ‚Üí nh·∫≠n session_id",
                "POST /continue": "Tell me more v·ªõi session_id"
            }
        },
        "quick_start": [
            "1. POST /upload v·ªõi file .txt",
            "2. POST /ask v·ªõi query v√† optional custom_prompt",
            "3. POST /continue v·ªõi session_id ƒë·ªÉ ƒë√†o s√¢u"
        ]
    }


@app.get(
    "/health",
    response_model=HealthResponse,
    tags=["üìä Info"],
    summary="Health check",
    description="""
## Ki·ªÉm tra tr·∫°ng th√°i h·ªá th·ªëng

### Ki·ªÉm tra:
- Elasticsearch connection
- Documents count
- Active sessions

### Status codes:
- `healthy`: H·ªá th·ªëng ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng
- `degraded`: C√≥ v·∫•n ƒë·ªÅ nh∆∞ng v·∫´n ho·∫°t ƒë·ªông
- `unhealthy`: H·ªá th·ªëng kh√¥ng ho·∫°t ƒë·ªông
    """
)
async def health():
    """Health check endpoint v·ªõi chi ti·∫øt v·ªÅ ES v√† sessions."""
    try:
        # Check Elasticsearch
        es_health = es.cluster.health()
        es_status = es_health["status"]
        es_connected = True
    except Exception as e:
        es_status = f"error: {str(e)}"
        es_connected = False
    
    doc_count = get_document_count()
    active_sessions = session_manager.get_active_count()
    
    # Determine overall status
    if es_connected and doc_count > 0:
        status = "healthy"
    elif es_connected:
        status = "degraded"
    else:
        status = "unhealthy"
    
    return HealthResponse(
        status=status,
        elasticsearch=es_status,
        elasticsearch_connected=es_connected,
        documents_indexed=doc_count,
        active_sessions=active_sessions,
        ready=doc_count > 0,
        message="Upload a file with POST /upload to get started" if doc_count == 0 else "System ready for queries"
    )
