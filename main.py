# main.py
"""
AI Vector Search Demo (Elasticsearch)
=====================================
Full-featured Q&A system with:
- Multi-level retrieval (Level 0 â†’ 1 â†’ 2 â†’ 3)
  - Level 0: Keyword combinations
  - Level 1: Single keywords
  - Level 2: Synonyms
  - Level 3: Keyword + Magical words
- Structured prompt builder with custom prompts
- "Tell me more" functionality with progressive exploration
- File management (upload with streaming, replace, delete)
- Buffer 10-20% for better retrieval
"""
import os
import uuid
import logging
from datetime import datetime
from typing import List
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

from config import settings
from vector.elastic_client import init_index, es
from services.splitter import split_into_sentences
from services.retriever import (
    index_sentences, 
    index_sentences_batch,
    get_top_unique_sentences_grouped,
    get_sentences_by_level,
    get_max_level,
    delete_all_documents,
    get_document_count
)
from services.prompt_builder import (
    generate_question_variants,
    extract_keywords,
    build_final_prompt,
    call_llm,
)
from services.session_manager import session_manager
from services.keyword_extractor import extract_keywords as extract_clean_keywords
from services.multi_level_retriever import get_next_batch
from models.request_models import (
    AskRequest, 
    AskResponse, 
    ContinueRequest, 
    ContinueResponse,
    UploadResponse,
    DocumentStats,
    HealthResponse,
    ErrorResponse
)

app = FastAPI(
    title="AI Vector Search Demo (Elasticsearch)",
    description="""
## ðŸ¤– Intelligent Q&A System with Multi-level Retrieval

### ðŸ“‹ Overview
This system uses Elasticsearch as a vector database, OpenAI for embeddings and chat.
Supports **7 main modules** as per client requirements:

### âœ… Modules:
1. **File Upload** - Upload .txt files with streaming (prevents RAM overflow)
2. **Sentence Embeddings** - Create vector embeddings for each sentence
3. **Query Processing** - Process queries with 10-20% buffer
4. **Deduplication** - Remove duplicate sentences
5. **Prompt Builder** - Build structured prompts + custom prompts
6. **Response Generation** - Generate answers from LLM
7. **"Tell me more"** - Dive deeper into subsequent levels

### ðŸ”„ Workflow:
```
POST /upload â†’ Split sentences â†’ Embedding â†’ Store in ES (by level)
POST /ask â†’ Vector search + Buffer â†’ Prompt Builder â†’ LLM â†’ Response + session_id
POST /continue â†’ Use session_id â†’ Next level â†’ Expand answer
```

### ðŸ’¡ Key Features:
- **Buffer 10-20%**: Retrieve extra sentences to improve results
- **Custom Prompts**: Users can add their own instructions
- **Streaming Upload**: Read files in chunks to prevent RAM overflow
- **Session Management**: Track conversations for "Tell me more"
    """,
    version="2.0.0",
    openapi_tags=[
        {
            "name": "ðŸ“ File Management",
            "description": "Upload, replace, and manage documents in Elasticsearch"
        },
        {
            "name": "â“ Q&A",
            "description": "Q&A with multi-level retrieval and custom prompts"
        },
        {
            "name": "ðŸ“Š Info",
            "description": "Health check and system information"
        }
    ],
    responses={
        400: {"model": ErrorResponse, "description": "Bad Request"},
        404: {"model": ErrorResponse, "description": "Not Found"},
        500: {"model": ErrorResponse, "description": "Internal Server Error"}
    }
)

# CORS for easy frontend testing
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


# Initialize index on app startup
@app.on_event("startup")
def startup_event():
    try:
        init_index()
    except Exception as e:
        print(f"Warning: Could not initialize Elasticsearch index: {e}")
        print("Server will continue without Elasticsearch connection.")


# ============================================================
# MODULE 1: File Management
# ============================================================

CHUNK_SIZE = 1024 * 1024  # 1MB chunks for streaming


@app.post(
    "/upload",
    response_model=UploadResponse,
    tags=["ðŸ“ File Management"],
    summary="Upload and index .txt file",
    description="""
## Upload Text File

### âš™ï¸ Processing:
1. **Streaming read** - Read file in chunks (1MB) to prevent RAM overflow
2. **Sentence splitting** - Split into individual sentences
3. **Level assignment** - Every 5 sentences = 1 level
4. **Batch embedding** - Create embeddings in batches (efficient)
5. **Elasticsearch indexing** - Store in vector database

### ðŸ“Š Response:
- `file_id`: Unique file ID
- `filename`: Original filename
- `total_sentences`: Number of indexed sentences
- `max_level`: Highest level (indicates how many levels for "Tell me more")
- `buffer_info`: Buffer capability information

### âš ï¸ Notes:
- Only `.txt` files supported
- Encoding: UTF-8 or Latin-1 (auto-detect)
    """,
    responses={
        200: {
            "description": "File uploaded successfully",
            "content": {
                "application/json": {
                    "example": {
                        "file_id": "550e8400-e29b-41d4-a716-446655440000",
                        "filename": "document.txt",
                        "total_sentences": 50,
                        "max_level": 10,
                        "message": "File processed successfully. 50 sentences indexed across 11 levels."
                    }
                }
            }
        },
        400: {"description": "Invalid file type or empty file"}
    }
)
async def upload_file(
    file: UploadFile = File(
        ..., 
        description="The .txt file to upload. Recommended max size: 200MB"
    ),
    split_mode: str = Query(
        default="auto",
        description="How to split text: 'auto' (detect), 'line' (per-line for Bible/verses), 'nltk' (paragraphs)",
        enum=["auto", "line", "nltk"]
    )
):
    """
    Upload .txt file with streaming read for RAM optimization.
    
    **split_mode options:**
    - `auto`: Auto-detect based on file structure
    - `line`: Split by newlines (for Bible, verse-per-line files) 
    - `nltk`: Use NLTK sentence tokenizer (for paragraph-style text)
    
    **Supported encodings:** UTF-8, UTF-16, Latin-1, CP1252 (Windows)
    """
    # Allow any text file extension
    allowed_extensions = [".txt", ".text", ".md", ".csv", ".log", ".dat"]
    file_ext = "." + file.filename.split(".")[-1].lower() if "." in file.filename else ""
    
    if file_ext and file_ext not in allowed_extensions:
        # Just warn, don't block - try to process anyway
        print(f"[Upload] Warning: Unusual file extension '{file_ext}', will try to process as text")

    # Streaming read to prevent RAM overflow with large files
    chunks = []
    total_size = 0
    MAX_SIZE = 200 * 1024 * 1024  # 200MB limit
    
    try:
        while True:
            chunk = await file.read(CHUNK_SIZE)
            if not chunk:
                break
            chunks.append(chunk)
            total_size += len(chunk)
            
            if total_size > MAX_SIZE:
                raise HTTPException(
                    status_code=400,
                    detail=f"File too large. Maximum size is 200MB. Your file: {total_size / (1024*1024):.1f}MB"
                )
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail=f"Error reading file: {str(e)}"
        )
    
    if not chunks:
        raise HTTPException(
            status_code=400,
            detail="File is empty or could not be read."
        )
    
    content_bytes = b"".join(chunks)
    
    # Try multiple encodings - cp1252 first for Windows files with smart quotes
    text = None
    best_encoding = None
    
    # First try strict UTF-8
    try:
        text = content_bytes.decode("utf-8")
        # Validate: check if result has mostly printable chars
        printable_ratio = sum(1 for c in text[:1000] if c.isprintable() or c in '\n\r\t') / min(len(text), 1000)
        if printable_ratio > 0.95:
            best_encoding = "utf-8"
            print(f"[Upload] Successfully decoded with UTF-8 (printable ratio: {printable_ratio:.2%})")
        else:
            text = None  # Reset, try other encodings
    except UnicodeDecodeError:
        pass
    
    # If UTF-8 failed or gave bad results, try Windows encodings
    if text is None:
        for encoding in ["cp1252", "utf-8-sig", "utf-16", "iso-8859-1", "latin-1"]:
            try:
                candidate = content_bytes.decode(encoding)
                # Validate result
                printable_ratio = sum(1 for c in candidate[:1000] if c.isprintable() or c in '\n\r\t') / min(len(candidate), 1000)
                if printable_ratio > 0.90:
                    text = candidate
                    best_encoding = encoding
                    print(f"[Upload] Successfully decoded with {encoding} (printable ratio: {printable_ratio:.2%})")
                    break
            except (UnicodeDecodeError, LookupError):
                continue
    
    if text is None:
        # Last resort: decode with errors='replace' to replace bad chars with ?
        text = content_bytes.decode("utf-8", errors="replace")
        print(f"[Upload] Warning: Used fallback decoding with character replacement")
    
    # Clean up text: remove null bytes, normalize line endings
    text = text.replace("\x00", "")  # Remove null bytes
    text = text.replace("\r\n", "\n").replace("\r", "\n")  # Normalize line endings
    
    # Remove BOM if present
    if text.startswith("\ufeff"):
        text = text[1:]
    
    try:
        sentences = split_into_sentences(text, split_mode=split_mode)
        print(f"[Upload] Split mode: {split_mode}, Total sentences: {len(sentences)}")
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail=f"Error splitting text into sentences: {str(e)}"
        )
    
    if not sentences:
        raise HTTPException(
            status_code=400, 
            detail="No valid sentences found in file. Make sure the file contains readable text."
        )

    # Index sentences with batch processing
    try:
        file_id = str(uuid.uuid4())
        max_level = index_sentences_batch(sentences, file_id=file_id, batch_size=500)
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error indexing sentences: {str(e)}"
        )

    return UploadResponse(
        file_id=file_id,
        filename=file.filename,
        total_sentences=len(sentences),
        max_level=max_level,
        message=f"File processed successfully. {len(sentences)} sentences indexed across {max_level + 1} levels.",
        buffer_info=f"With 15% buffer, queries can retrieve up to {int(15 * 1.15)} sentences"
    )


@app.post(
    "/replace",
    response_model=UploadResponse,
    tags=["ðŸ“ File Management"],
    summary="Replace all data",
    description="""
## Replace Current Data with New File

### âš™ï¸ Processing:
1. **Delete all** old documents in Elasticsearch
2. **Upload and index** new file

### âš ï¸ Warning:
- This action CANNOT be undone
- All current sessions will be invalidated
    """
)
async def replace_file(
    file: UploadFile = File(..., description="New .txt file to replace current data")
):
    """Replace all data with new file."""
    delete_all_documents()
    session_manager.clear_all()
    return await upload_file(file)


@app.delete(
    "/documents",
    tags=["ðŸ“ File Management"],
    summary="Delete all documents",
    description="""
## Delete All Data

### âš ï¸ Warning:
- This action CANNOT be undone
- You need to upload a new file before using /ask
    """,
    responses={
        200: {
            "description": "Deleted successfully",
            "content": {
                "application/json": {
                    "example": {"message": "All documents deleted successfully", "documents_deleted": 50}
                }
            }
        }
    }
)
async def delete_all():
    """Delete all documents in Elasticsearch."""
    count = get_document_count()
    success = delete_all_documents()
    session_manager.clear_all()
    
    if success:
        return {"message": "All documents deleted successfully", "documents_deleted": count}
    raise HTTPException(status_code=500, detail="Failed to delete documents")


@app.get(
    "/documents/count",
    response_model=DocumentStats,
    tags=["ðŸ“ File Management"],
    summary="Get document statistics",
    description="""
## Document Statistics in Elasticsearch

### ðŸ“Š Returns:
- `total_documents`: Total indexed sentences
- `max_level`: Highest level
- `levels_available`: Number of levels for "Tell me more"
- `ready`: True if data exists, ready to accept queries
    """
)
async def get_count():
    """Get current document statistics."""
    count = get_document_count()
    max_level = get_max_level()
    return DocumentStats(
        total_documents=count,
        max_level=max_level,
        levels_available=max_level + 1 if count > 0 else 0,
        ready=count > 0
    )


# ============================================================
# MODULE 2-6: Ask Question (First Query)
# ============================================================

@app.post(
    "/ask",
    response_model=AskResponse,
    tags=["â“ Q&A"],
    summary="Ask a question",
    description="""
## Q&A with Multi-level Retrieval

### ðŸ”„ Processing Flow:
1. **Vector Search** - Find relevant source sentences (with 10-20% buffer)
2. **Deduplicate** - Remove duplicate sentences
3. **Generate Variants** - Create 3-4 question variants
4. **Extract Keywords** - Extract and explain important keywords
5. **Build Prompt** - Build structured prompt + custom instructions
6. **Call LLM** - Generate answer

### ðŸ“¥ Parameters:
- `query` (required): User's question
- `limit`: Maximum source sentences (default: 15)
- `buffer_percentage`: Extra sentences percentage (10-20%)
- `custom_prompt`: User's custom instructions

### ðŸ“¤ Response:
- `session_id`: Use for /continue (Tell me more)
- `answer`: LLM's answer
- `source_sentences`: Source sentences used
- `can_continue`: True if can explore deeper

### ðŸ’¡ Tips:
- Use `buffer_percentage=15` for balance between accuracy and diversity
- Add `custom_prompt` to adjust answer style/format
    """,
    responses={
        200: {
            "description": "Answer generated successfully",
            "content": {
                "application/json": {
                    "example": {
                        "session_id": "abc-123",
                        "answer": "Based on the information found...",
                        "question_variants": "1. What is X?\n2. Explain X...",
                        "source_sentences": [{"text": "Sample sentence", "level": 0, "score": 0.95}],
                        "current_level": 0,
                        "max_level": 5,
                        "can_continue": True
                    }
                }
            }
        }
    }
)
async def ask(req: AskRequest):
    """
    Receive user question and execute full flow with MULTI-LEVEL retrieval.
    
    Level 0: Keyword combinations (most specific â†’ least specific)
    Level 1: Single keyword search
    Level 2: Synonym-based search
    Level 3: Keyword + Magical words combinations
    """
    logger.info(f"[API /ask] New request - query='{req.query}', limit={req.limit}")
    
    # Check if data exists
    if get_document_count() == 0:
        raise HTTPException(
            status_code=404, 
            detail="No documents found. Please upload a file first using POST /upload"
        )
    
    # Step 1: Extract clean keywords (filtered from magic words)
    clean_keywords = extract_clean_keywords(req.query)
    
    logger.info(f"[API /ask] Extracted keywords: {clean_keywords}")
    print(f"[DEBUG] Query: '{req.query}' â†’ Keywords extracted: {clean_keywords}")
    
    if not clean_keywords:
        # Fallback: use simple word extraction
        clean_keywords = [w for w in req.query.lower().split() if len(w) > 3][:5]
        print(f"[DEBUG] Fallback keywords: {clean_keywords}")
    
    # Step 2: Get first batch of sentences using multi-level retrieval
    # Count meaningful words in original query (excluding stopwords and question words)
    stopwords = {'what', 'where', 'when', 'who', 'why', 'how', 'which', 'whom', 'whose',
                 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                 'the', 'a', 'an', 'and', 'or', 'but', 'if', 'so', 'as',
                 'to', 'of', 'in', 'on', 'at', 'by', 'for', 'with', 'about',
                 'do', 'does', 'did', 'can', 'could', 'will', 'would', 'should', 'may', 'might',
                 'this', 'that', 'these', 'those', 'it', 'its'}
    
    # Extract meaningful nouns from query
    query_words = [w.lower().strip('?!.,;:') for w in req.query.split()]
    meaningful_query_words = [w for w in query_words if w not in stopwords and len(w) > 2]
    
    print(f"[DEBUG] Meaningful words in query: {meaningful_query_words}")
    
    # If query has only 1 meaningful word â†’ start from Level 1 (keyword + magical words)
    # NEW LOGIC: Level 1 = keyword + magic words (e.g., "heaven is")
    if len(meaningful_query_words) <= 1:
        # Single keyword query â†’ start at Level 1 for contextual search
        initial_state = {
            "current_level": 1,  # Changed from 3 to 1
            "level_offsets": {"0": 0, "1": 0, "2": [0, 0], "3": 0},
            "used_sentence_ids": []
        }
        print(f"[INFO] Only 1 meaningful word found â†’ Starting from Level 1 (keyword + magic words)")
    else:
        initial_state = {
            "current_level": 0,
            "level_offsets": {"0": 0, "1": 0, "2": [0, 0], "3": 0},
            "used_sentence_ids": []
        }
    
    source_sentences, updated_state, level_used = get_next_batch(
        session_state=initial_state,
        keywords=clean_keywords,
        batch_size=req.limit if req.limit else 15,
        enabled_levels=req.enabled_levels if req.enabled_levels else None
    )
    
    if not source_sentences:
        # Fallback to old method if multi-level returns nothing
        logger.warning(f"[API /ask] Multi-level retrieval returned no results, using fallback")
        source_sentences = get_top_unique_sentences_grouped(
            req.query, 
            limit=req.limit,
            buffer_percentage=req.buffer_percentage
        )
    
    logger.info(f"[API /ask] Retrieved {len(source_sentences)} source sentences")
    
    if not source_sentences:
        raise HTTPException(
            status_code=404, 
            detail="No source sentences found matching your query. Try rephrasing your question."
        )

    # Step 3: Generate question variants + extract keyword meaning
    question_variants = generate_question_variants(req.query)
    
    # Use pre-provided keyword_meaning if available, otherwise generate via LLM
    if req.keyword_meaning:
        keyword_meaning = req.keyword_meaning
        print(f"[INFO] Using pre-provided keyword_meaning")
    else:
        keyword_meaning = extract_keywords(req.query)
        print(f"[INFO] Generated keyword_meaning via LLM")

    # Step 4: Build final prompt with custom_prompt support
    prompt = build_final_prompt(
        user_query=req.query,
        question_variants=question_variants,
        keyword_meaning=keyword_meaning,
        source_sentences=source_sentences,
        continue_mode=False,
        custom_prompt=req.custom_prompt
    )

    # Step 5: Call LLM
    answer = call_llm(prompt)
    
    # Step 6: Create session with keywords and level tracking
    session = session_manager.create_session(
        query=req.query, 
        max_level=20,  # 21 levels: 0-20 for deep testing
        keywords=clean_keywords
    )
    
    # Update session with state from retriever
    session_manager.update_session(
        session.session_id,
        used_sentences=[s["text"] for s in source_sentences],
        question_variants=question_variants,
        keywords=keyword_meaning,
        state_dict=updated_state
    )
    
    # Calculate current_level from state
    current_level = updated_state.get("current_level", 0)
    
    # can_continue = True if current_level < 20 (still have levels to explore)
    can_continue = current_level <= 20

    return AskResponse(
        session_id=session.session_id,
        answer=answer,
        question_variants=question_variants,
        keywords=clean_keywords,  # Add extracted keywords list
        keyword_meaning=keyword_meaning,
        source_sentences=source_sentences,
        current_level=level_used,
        max_level=20,
        prompt_used=prompt,
        can_continue=can_continue,
        sentences_retrieved=len(source_sentences),
        buffer_applied=req.buffer_percentage if req.buffer_percentage else 0
    )


# ============================================================
# MODULE 7: Continue / Tell me more
# ============================================================

@app.post(
    "/continue",
    response_model=ContinueResponse,
    tags=["â“ Q&A"],
    summary="Tell me more - Explore deeper",
    description="""
## Expand Answer with Information from Deeper Levels

### ðŸ”„ Processing Flow:
1. **Get Session** - Retrieve info from session_id
2. **Increase Level** - Move to Level 1, 2, 3...
3. **Get NEW sentences** - Get NEW source sentences (exclude used ones)
4. **Generate NEW variants** - Create NEW question variants
5. **Update Keywords** - Add new keywords
6. **Build Prompt** - Prompt for continue mode + custom instructions
7. **Call LLM** - Generate expanded answer

### ðŸ“¥ Parameters:
- `session_id` (required): ID from /ask response
- `custom_prompt`: Additional custom instructions
- `buffer_percentage`: Extra sentences percentage (10-20%)

### ðŸ“¤ Response:
- Similar to /ask but with info from deeper levels
- `can_continue`: False when all levels explored

### ðŸ’¡ Usage Pattern:
```
1. POST /ask â†’ get session_id
2. POST /continue with session_id â†’ get more info
3. Repeat POST /continue until can_continue=false
```
    """,
    responses={
        200: {"description": "Expanded answer generated successfully"},
        404: {"description": "Session not found or expired"},
        400: {"description": "No more levels to explore"}
    }
)
async def continue_conversation(req: ContinueRequest):
    """
    "Tell me more" - Progressive exploration using multi-level retrieval.
    
    Automatically moves through levels:
    Level 0 â†’ Level 1 â†’ Level 2 â†’ Level 3
    
    Each call fetches next batch of sentences, avoiding previously used ones.
    """
    logger.info(f"[API /continue] Session={req.session_id}, limit={req.limit}")
    
    # Get session
    session = session_manager.get_session(req.session_id)
    if not session:
        logger.warning(f"[API /continue] Session not found: {req.session_id}")
        raise HTTPException(
            status_code=404,
            detail="Session not found or expired (30 min timeout). Please ask a new question with POST /ask"
        )
    
    # Check if can continue (level 0-3)
    if session.current_level > 3:
        raise HTTPException(
            status_code=400,
            detail="No more levels available. All information has been explored. Start a new question with POST /ask"
        )
    
    # Get session state for retriever
    session_state = session.get_state_dict()
    
    # Use stored keywords from session
    keywords = session.keywords if session.keywords else [w for w in session.original_query.lower().split() if len(w) > 3][:5]
    
    # Get next batch using multi-level retriever
    source_sentences, updated_state, level_used = get_next_batch(
        session_state=session_state,
        keywords=keywords,
        batch_size=req.limit if req.limit else 15
    )
    
    # If no more sentences, return response with can_continue=False
    if not source_sentences:
        return ContinueResponse(
            session_id=session.session_id,
            answer="All available information has been explored. Please start a new conversation with a different question.",
            question_variants=[],
            keywords=session.keywords if hasattr(session, 'keywords') and session.keywords else [],
            keyword_meaning="All keywords have been fully explored.",
            source_sentences=[],
            current_level=updated_state.get("current_level", 21),
            max_level=20,
            prompt_used="",
            can_continue=False,
            sentences_retrieved=0,
            buffer_applied=0
        )
    
    # Generate NEW question variants (deeper exploration)
    question_variants = generate_question_variants(
        session.original_query,
        previous_variants=session.used_variants,
        continue_mode=True
    )
    
    # Generate deeper keyword meaning
    keyword_meaning = extract_keywords(
        session.original_query,
        previous_keywords=session.previous_keywords,
        continue_mode=True
    )
    
    # Build new prompt for deeper exploration
    prompt = build_final_prompt(
        user_query=session.original_query,
        question_variants=question_variants,
        keyword_meaning=keyword_meaning,
        source_sentences=source_sentences,
        continue_mode=True,
        continue_count=session.continue_count + 1,
        custom_prompt=req.custom_prompt
    )
    
    # Call LLM
    answer = call_llm(prompt)
    
    # Update session with new state
    session_manager.update_session(
        session.session_id,
        used_sentences=[s["text"] for s in source_sentences],
        question_variants=question_variants,
        keywords=keyword_meaning,
        increment_level=True,
        state_dict=updated_state
    )
    
    # Get current level from updated state
    current_level = updated_state.get("current_level", level_used)
    
    # can_continue = True if there are still levels to explore
    can_continue = current_level <= 20
    
    return ContinueResponse(
        session_id=session.session_id,
        answer=answer,
        question_variants=question_variants,
        keywords=session.keywords if hasattr(session, 'keywords') and session.keywords else [],
        keyword_meaning=keyword_meaning,
        source_sentences=source_sentences,
        current_level=level_used,
        max_level=20,
        prompt_used=prompt,
        can_continue=can_continue,
        continue_count=session.continue_count + 1,
        sentences_retrieved=len(source_sentences),
        buffer_applied=req.buffer_percentage if req.buffer_percentage else 0
    )


# ============================================================
# Health & Info Endpoints
# ============================================================

@app.get(
    "/",
    tags=["ðŸ“Š Info"],
    summary="API Information",
    description="Returns overview information about the API and endpoints"
)
async def root():
    """API overview information."""
    return {
        "message": "ðŸ¤– AI Vector Search Demo with Elasticsearch",
        "version": "2.0.0",
        "docs": "/docs",
        "redoc": "/redoc",
        "features": [
            "âœ… Multi-level retrieval (Level 0, 1, 2...)",
            "âœ… Structured prompt builder",
            "âœ… Custom prompts support",
            "âœ… Buffer 10-20% for better retrieval",
            "âœ… Tell me more functionality",
            "âœ… Streaming file upload",
            "âœ… File management"
        ],
        "endpoints": {
            "file_management": {
                "POST /upload": "Upload .txt file (streaming)",
                "POST /replace": "Replace all data",
                "DELETE /documents": "Delete all",
                "GET /documents/count": "Get document statistics"
            },
            "qa": {
                "POST /ask": "Ask question â†’ get session_id",
                "POST /continue": "Tell me more with session_id"
            }
        },
        "quick_start": [
            "1. POST /upload with .txt file",
            "2. POST /ask with query and optional custom_prompt",
            "3. POST /continue with session_id to explore deeper"
        ]
    }


@app.get(
    "/health",
    response_model=HealthResponse,
    tags=["ðŸ“Š Info"],
    summary="Health check",
    description="""
## Check System Status

### Checks:
- Elasticsearch connection
- Documents count
- Active sessions

### Status codes:
- `healthy`: System operating normally
- `degraded`: Issues but still operational
- `unhealthy`: System not operational
    """
)
async def health():
    """Health check endpoint with ES and session details."""
    try:
        es_health = es.cluster.health()
        es_status = es_health["status"]
        es_connected = True
    except Exception as e:
        es_status = f"error: {str(e)}"
        es_connected = False
    
    doc_count = get_document_count()
    active_sessions = session_manager.get_active_count()
    
    if es_connected and doc_count > 0:
        status = "healthy"
    elif es_connected:
        status = "degraded"
    else:
        status = "unhealthy"
    
    return HealthResponse(
        status=status,
        elasticsearch=es_status,
        elasticsearch_connected=es_connected,
        documents_indexed=doc_count,
        active_sessions=active_sessions,
        ready=doc_count > 0,
        message="Upload a file with POST /upload to get started" if doc_count == 0 else "System ready for queries"
    )


# ============================================================
# DEBUG ENDPOINTS (for testing)
# ============================================================

@app.get(
    "/debug/keywords",
    tags=["ðŸ”§ Debug"],
    summary="Debug: Extract and analyze keywords",
    description="Extract keywords from query and show filtering details"
)
async def debug_keywords(query: str):
    """Debug endpoint to see keyword extraction details."""
    from services.keyword_extractor import (
        extract_keywords_raw,
        filter_magic_words,
        extract_keywords,
        generate_keyword_combinations,
        generate_synonyms,
        generate_keyword_magical_pairs,
        MAGIC_WORDS
    )
    
    raw = extract_keywords_raw(query)
    filtered = filter_magic_words(raw)
    final = extract_keywords(query)
    combinations = generate_keyword_combinations(final)
    
    # Get synonyms for each keyword
    synonyms = {}
    for kw in final[:3]:  # Limit to avoid too many API calls
        synonyms[kw] = generate_synonyms(kw)
    
    # Get magical pairs
    magical_pairs = generate_keyword_magical_pairs(final[:2])[:10]  # Limit
    
    return {
        "query": query,
        "raw_keywords": raw,
        "filtered_keywords": filtered,
        "final_keywords": final,
        "magic_words_count": len(MAGIC_WORDS),
        "combinations": [list(c) for c in combinations[:10]],
        "synonyms": synonyms,
        "magical_pairs": [list(p) for p in magical_pairs],
    }


@app.get(
    "/debug/level/{level}",
    tags=["ðŸ”§ Debug"],
    summary="Debug: Test specific level retrieval",
    description="Fetch sentences from a specific level only"
)
async def debug_level(
    level: int,
    query: str,
    limit: int = 10
):
    """Debug endpoint to test each level independently."""
    from services.keyword_extractor import extract_keywords
    from services.multi_level_retriever import MultiLevelRetriever
    
    keywords = extract_keywords(query)
    if not keywords:
        keywords = [w for w in query.lower().split() if len(w) > 3][:5]
    
    retriever = MultiLevelRetriever(keywords)
    used_texts = set()
    
    if level == 0:
        sentences, offset, exhausted = retriever.fetch_level0_sentences(
            offset=0, limit=limit, used_texts=used_texts
        )
        return {
            "level": 0,
            "strategy": "keyword_combinations",
            "keywords": keywords,
            "sentences": sentences,
            "offset": offset,
            "exhausted": exhausted
        }
    
    elif level == 1:
        sentences, offset, exhausted = retriever.fetch_level1_sentences(
            offset=0, limit=limit, used_texts=used_texts
        )
        return {
            "level": 1,
            "strategy": "single_keywords",
            "keywords": keywords,
            "sentences": sentences,
            "offset": offset,
            "exhausted": exhausted
        }
    
    elif level == 2:
        sentences, k_off, s_off, exhausted = retriever.fetch_level2_sentences(
            keyword_offset=0, synonym_offset=0, limit=limit, used_texts=used_texts
        )
        return {
            "level": 2,
            "strategy": "synonyms",
            "keywords": keywords,
            "sentences": sentences,
            "keyword_offset": k_off,
            "synonym_offset": s_off,
            "exhausted": exhausted
        }
    
    elif level == 3:
        sentences, offset, exhausted = retriever.fetch_level3_sentences(
            offset=0, limit=limit, used_texts=used_texts
        )
        return {
            "level": 3,
            "strategy": "keyword_magical_pairs",
            "keywords": keywords,
            "sentences": sentences,
            "offset": offset,
            "exhausted": exhausted
        }
    
    else:
        return {"error": f"Invalid level {level}. Valid: 0, 1, 2, 3"}


@app.get(
    "/debug/session/{session_id}",
    tags=["ðŸ”§ Debug"],
    summary="Debug: View session state",
    description="View current session state including level offsets and used sentences"
)
async def debug_session(session_id: str):
    """Debug endpoint to inspect session state."""
    session = session_manager.get_session(session_id)
    
    if not session:
        return {"error": "Session not found or expired"}
    
    return {
        "session_id": session.session_id,
        "original_query": session.original_query,
        "current_level": session.current_level,
        "max_level_available": session.max_level_available,
        "level_offsets": session.level_offsets,
        "keywords": session.keywords,
        "used_sentences_count": len(session.used_sentences),
        "used_variants_count": len(session.used_variants),
        "continue_count": session.continue_count,
        "created_at": session.created_at.isoformat(),
        "last_accessed": session.last_accessed.isoformat()
    }
